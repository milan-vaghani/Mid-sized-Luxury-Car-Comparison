{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a06735-2055-4bd7-b015-d4a44d9328f1",
   "metadata": {},
   "source": [
    "# Analytics For Unstructured Data: Group Assignment #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311e102-1f3d-43d0-a7c0-64321bff3b96",
   "metadata": {},
   "source": [
    "## Scraper\n",
    "https://realpython.com/modern-web-automation-with-python-and-selenium/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22220f8c-4c47-4fa5-ac6b-af5516b13d83",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /opt/anaconda3/lib/python3.12/site-packages (4.24.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (2024.6.2)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "zsh:1: command not found: apt-get\n",
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "cp: /usr/lib/chromium-browser/chromedriver: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Use this code if using Colab to run Selenium\n",
    "\n",
    "# Make sure to go to Runtime -> Change runtime and set GPU as hardware accelerator\n",
    "\n",
    "# !kill -9 -1 # Use this line to delete this VM and start a new one.\n",
    "# The above line deletes all files and folders from the current VM and allocates a new one.\n",
    "\n",
    "#Selenium is an open-source tool that automates web browsers.\n",
    "!pip install selenium\n",
    "!apt-get -q update   #Used to handle installation and removal of softwares and libraries\n",
    "!apt install -yq chromium-chromedriver #ChromeDriver is a separate executable that Selenium WebDriver uses to control Chrome.\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import operator\n",
    "import io\n",
    "import csv\n",
    "import re\n",
    "import decimal\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from itertools import combinations\n",
    "import shutil\n",
    "from tempfile import NamedTemporaryFile\n",
    "#WebDriver is a browser automation framework that works with open source APIs.\n",
    "#The framework operates by accepting commands, sending those commands to a browser, and interacting with applications.\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "#headless means running chrome with chrome.exe\n",
    "chrome_options.add_argument('--headless')\n",
    "\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba04912b-81b2-409a-836b-e78e813ff1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Create DataFrame\n",
    "comments = pd.DataFrame(columns = ['Date','user_id','comments'])\n",
    "\n",
    "\n",
    "#Srape Dates, Usernames, and Comments from most recent 115 pages (about 5000 comments)\n",
    "for i in range(320,436):\n",
    "\n",
    "    #address where scraping\n",
    "    webpage = 'https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p' + str(i)\n",
    "    driver.get(webpage)\n",
    "\n",
    "    ids = driver.find_elements(By.XPATH,\"//*[contains(@id,'Comment_')]\")\n",
    "\n",
    "    comment_ids = []\n",
    "\n",
    "    for i in ids:\n",
    "        comment_ids.append(i.get_attribute('id'))\n",
    "\n",
    "    #check if there is a blockquote (used in replies to comments) and remove\n",
    "    for x in comment_ids:\n",
    "        try:\n",
    "            element = driver.find_elements(By.XPATH,'//*[@id=\"' + x +'\"]/div/div[3]/div/div[1]/blockquote')[0]\n",
    "            driver.execute_script(\"\"\"\n",
    "                var element = arguments[0];\n",
    "                element.parentNode.removeChild(element);\n",
    "                \"\"\", element)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for x in comment_ids:\n",
    "\n",
    "        #Extract dates from for each user on a page\n",
    "        user_date = driver.find_elements(By.XPATH,'//*[@id=\"' + x +'\"]/div/div[2]/div[2]/span[1]/a/time')[0]\n",
    "        date = user_date.get_attribute('title')\n",
    "\n",
    "        #Extract user ids from each user on a page\n",
    "        userid_element = driver.find_elements(By.XPATH,'//*[@id=\"' + x +'\"]/div/div[2]/div[1]/span[1]/a[2]')[0]\n",
    "        userid = userid_element.text\n",
    "\n",
    "        #Extract Message for each user on a page\n",
    "        user_message = driver.find_elements(By.XPATH,'//*[@id=\"' + x +'\"]/div/div[3]/div/div[1]')[0]\n",
    "\n",
    "        comment = user_message.text\n",
    "\n",
    "\n",
    "        #Adding date, userid and comment for each user in a dataframe\n",
    "        comments.loc[len(comments)] = [date,userid,comment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d3fb2a-8d96-45a0-a93e-49db42028bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nulls don't play well with the tokenizer, so drop nulls\n",
    "comments.dropna(inplace=True)\n",
    "\n",
    "#change the location as per the file destination you want it to be\n",
    "comments.to_csv('data/Edmunds_scraped2.0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48e79dfa-8db2-4bbf-af41-f42adf2088b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If working from CSV instead of scraping originally, read in CSV\n",
    "comments = pd.read_csv('data/Edmunds_scraped2.0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9d01c-9d73-422c-b5db-e4d8446db2df",
   "metadata": {},
   "source": [
    "## Task A: Zipf's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "124ff038-0417-4248-8bf1-4ce50ae25f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/kenne/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word frequencies written to data/word_freq.csv\n"
     ]
    }
   ],
   "source": [
    "# raw word frequencies\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import io\n",
    "\n",
    "# Download stopwords from the NLTK package\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Input and output filenames\n",
    "input_filename = 'data/Edmunds_scraped2.0.csv'  # Input file\n",
    "final_filename = 'data/final.csv'  # Intermediate file without the column header\n",
    "word_freq_output = 'data/word_freq.csv'  # Output file for word frequencies\n",
    "\n",
    "# Function to clean and tokenize sentences\n",
    "def clean_and_tokenize(sentence):\n",
    "    \"\"\"\n",
    "    Cleans a given sentence by removing punctuation and stopwords, converting text to lowercase,\n",
    "    and tokenizing the remaining words.\n",
    "    \"\"\"\n",
    "    # Remove punctuation and convert text to lowercase\n",
    "    sentence = re.sub(f'[{re.escape(string.punctuation)}]', '', sentence.lower())\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    return [word for word in sentence.split() if word not in stop_words]\n",
    "\n",
    "# Step 1: Remove header from the input CSV and create a new file without it\n",
    "def remove_header(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Reads the input CSV file, removes the header, and writes the remaining rows\n",
    "    into a new output file.\n",
    "    \"\"\"\n",
    "    file = pd.read_csv(input_file, header = None, skiprows = 1)\n",
    "    file.to_csv(output_file, index = False)\n",
    "\n",
    "# Step 2: Extract and clean sentences from the text\n",
    "def extract_sentences(file):\n",
    "    \"\"\"\n",
    "    Extracts text data from the third column of the CSV file, splits it into sentences,\n",
    "    and cleans each sentence by removing punctuation and stopwords.\n",
    "    The result is a list with 3 levels: message, sentences, and words\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "    posts = df.iloc[:,2]\n",
    "    sentences = []\n",
    "    sentences_clean = []\n",
    "    for post in posts:\n",
    "        sentences.extend(re.split('[?.!]', post))\n",
    "    for sentence in sentences:\n",
    "        cleaned_tokens = clean_and_tokenize(sentence)\n",
    "        if cleaned_tokens:\n",
    "            sentences_clean.append(cleaned_tokens)\n",
    "    return sentences_clean\n",
    "\n",
    "\n",
    "# Step 3: Calculate word frequencies\n",
    "def calculate_word_frequencies(sentences):\n",
    "    \"\"\"\n",
    "    Calculates the frequency of each word in the given list of cleaned sentences.\n",
    "    \"\"\"\n",
    "    freqs = {}\n",
    "    total_words = 0\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            freqs[word] = freqs.get(word, 0) + 1\n",
    "            total_words += 1\n",
    "    return freqs\n",
    "\n",
    "# Step 4: Write word frequencies to CSV\n",
    "def write_word_frequencies(word_freq, output_file):\n",
    "    \"\"\"\n",
    "    Writes the word frequencies to the specified CSV file.\n",
    "    \"\"\"\n",
    "    word_freq_df = pd.DataFrame(word_freq.items(), columns = [\"Word\", \"Frequency\"])\n",
    "    word_freq_df = word_freq_df.sort_values(by = \"Frequency\", ascending = False)\n",
    "    word_freq_df.to_csv(output_file, index=False)\n",
    "    print(f\"Word frequencies written to {output_file}\")\n",
    "\n",
    "# Main function to run all steps\n",
    "def main():\n",
    "    # Step 1: Remove header\n",
    "    remove_header(input_filename, final_filename)\n",
    "    \n",
    "    # Step 2: Extract and clean sentences\n",
    "    sentences = extract_sentences(final_filename)\n",
    "    \n",
    "    # Step 3: Calculate word frequencies\n",
    "    word_freq = calculate_word_frequencies(sentences)\n",
    "    \n",
    "    # Step 4: Write word frequencies to CSV\n",
    "    write_word_frequencies(word_freq, word_freq_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8923e747-3f0f-4834-a99d-e439b664bca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>car</td>\n",
       "      <td>2715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like</td>\n",
       "      <td>1739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>would</td>\n",
       "      <td>1738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cars</td>\n",
       "      <td>1655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one</td>\n",
       "      <td>1599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20705</th>\n",
       "      <td>clatter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20706</th>\n",
       "      <td>allinall</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20707</th>\n",
       "      <td>pets</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20708</th>\n",
       "      <td>wnav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20709</th>\n",
       "      <td>explorerx4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20710 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word  Frequency\n",
       "0             car       2715\n",
       "1            like       1739\n",
       "2           would       1738\n",
       "3            cars       1655\n",
       "4             one       1599\n",
       "...           ...        ...\n",
       "20705     clatter          1\n",
       "20706    allinall          1\n",
       "20707        pets          1\n",
       "20708        wnav          1\n",
       "20709  explorerx4          1\n",
       "\n",
       "[20710 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/word_freq.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a30f067-ac20-4d81-ac3f-28210f5ab0fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56c5e0d0-6160-4e9b-9c53-07c0ea277f0b",
   "metadata": {},
   "source": [
    "## Task B: Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e054e-47be-4cde-bf65-310dfaffd715",
   "metadata": {},
   "source": [
    "### Find and Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8428331-f4dd-41fa-83fb-68d6f30dca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import shutil\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "# Filepaths\n",
    "output_file = 'data/find_and_replace.csv'  # The file where the modified data will be stored\n",
    "input_file = 'data/Edmunds_scraped2.0.csv'     # The file containing the original data\n",
    "replacement_file = 'data/car_models_and_brands.csv'  # The file containing original and replacement words\n",
    "\n",
    "# Create a temporary file to write the changes before moving it to the final location\n",
    "tempfile = NamedTemporaryFile(mode='w', delete=False, newline='', encoding='utf-8')\n",
    "\n",
    "def load_replacements(replacement_file):\n",
    "    \"\"\"\n",
    "    Load word replacements from a CSV file into a dictionary.\n",
    "    The right column contains words to be replaced by the corresponding words in the left column.\n",
    "    \"\"\"\n",
    "    replacements = {}\n",
    "    with open(replacement_file, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in reader:\n",
    "            original, replacement = row[1].lower(), row[0].lower()  # Ensure lowercase comparison\n",
    "            replacements[original] = replacement\n",
    "    return replacements\n",
    "\n",
    "def replace_words_in_text(text, replacements):\n",
    "    \"\"\"\n",
    "    Replace words in the input text according to the replacements dictionary.\n",
    "    \"\"\"\n",
    "    words = text.split()  # Split text into words\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        # Remove punctuation from the word\n",
    "        word_clean = re.sub(r'[^\\w\\s]', '', word).lower() \n",
    "        # Replace word if it's in the replacements dictionary\n",
    "        if word_clean in replacements:\n",
    "            new_word = replacements[word_clean]\n",
    "            # Preserve original word's punctuation and case\n",
    "            new_words.append(re.sub(word_clean, new_word, word, flags=re.IGNORECASE))\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def process_file(input_file, output_file, replacements):\n",
    "    \"\"\"\n",
    "    Read the input file, perform word replacements, and write the modified content to the output file.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, tempfile:\n",
    "        reader = csv.reader(infile, delimiter=',', quotechar='\"')\n",
    "        writer = csv.writer(tempfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        # Write the header if present\n",
    "        header = next(reader)\n",
    "        writer.writerow(header)\n",
    "\n",
    "        for row in reader:\n",
    "            new_row = [replace_words_in_text(col, replacements) for col in row]\n",
    "            writer.writerow(new_row)\n",
    "\n",
    "    # Move the temp file to the final output location\n",
    "    shutil.move(tempfile.name, output_file)\n",
    "\n",
    "def main():\n",
    "    # Load the replacement words from the replacement CSV file\n",
    "    replacements = load_replacements(replacement_file)\n",
    "    \n",
    "    # Process the input file and apply the replacements\n",
    "    process_file(input_file, output_file, replacements)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a28f9fc-1bbd-4e4b-8a30-87790032c608",
   "metadata": {},
   "source": [
    "### Frequency Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c94cb82-2235-42fb-b272-44c7909e0c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redo the frequency counts so you only count each word once per message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df997f4-d3de-4bef-bfe6-3def42a3e00f",
   "metadata": {},
   "source": [
    "## Task C: Lift Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9011513f-f8fd-4bf5-8208-48670ca11c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global variables and data structures\n",
    "df_lift = pd.DataFrame(columns=['word1', 'word2', 'lift_value'])  # To store lift values\n",
    "word_frequency = {}  # Dictionary to store word frequency in posts\n",
    "word_pair_frequency = defaultdict(dict)  # Dictionary to store word pair co-occurrence frequency\n",
    "results_dict = {}  # Dictionary to store results with lift values for word pairs\n",
    "file_length = 0  # Number of rows in the input file\n",
    "itr = 0  # Row iterator for the lift DataFrame\n",
    "\n",
    "# File paths\n",
    "input_file = 'data/find_and_replace.csv'  # Input data file\n",
    "pair_keys_file = 'edmunds_pair_keys.txt'  # File containing the words to calculate lift\n",
    "output_lift_values = 'data/Lift_Values.csv'  # Output file for lift values\n",
    "output_lift_matrix = 'data/Lift_Matrix.csv'  # Output file for lift matrix\n",
    "\n",
    "# Load NLTK stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean and tokenize text (removes punctuation and stopwords)\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans a given text by removing punctuation, converting it to lowercase,\n",
    "    and tokenizing it, ignoring any stopwords.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Step 1: Load the words from the edmunds_pair_keys.txt file and generate all pairs\n",
    "def load_word_pairs(filename):\n",
    "    \"\"\"\n",
    "    Loads words from a file where words are comma-separated in each row.\n",
    "    Returns a list of all possible word pairs for each row.\n",
    "    \"\"\"\n",
    "    word_pairs = []\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Generate all possible word pairs from each row\n",
    "            pairs = list(combinations(row, 2))\n",
    "            word_pairs.extend(pairs)\n",
    "\n",
    "    return word_pairs\n",
    "\n",
    "# Step 2: Process the input CSV file to extract posts and clean the text\n",
    "def process_input_file(input_filename):\n",
    "    \"\"\"\n",
    "    Processes the input CSV file to extract and clean posts. Each post is tokenized,\n",
    "    cleaned of punctuation and stopwords, and stored in a list.\n",
    "    \"\"\"\n",
    "    posts = []\n",
    "    global file_length\n",
    "    df = pd.read_csv(input_filename)  # Load the CSV file into a DataFrame\n",
    "\n",
    "    # Assuming 'comments' is the column that contains the text\n",
    "    for index, row in df.iterrows():\n",
    "        cleaned_post = clean_text(row['comments'])  # Clean and tokenize the post\n",
    "        posts.append(cleaned_post)\n",
    "\n",
    "    file_length = len(df)  # Get the total number of rows\n",
    "    return posts\n",
    "\n",
    "# Step 3: Calculate word frequencies and word pair co-occurrences (distance ≥ 5 words)\n",
    "def calculate_frequencies(posts):\n",
    "    \"\"\"\n",
    "    Calculates the frequency of individual words and word pairs within the posts.\n",
    "    Updates the global word_frequency and word_pair_frequency dictionaries.\n",
    "    Only considers word pairs that are 5 or more words apart.\n",
    "    \"\"\"\n",
    "    global word_frequency, word_pair_frequency\n",
    "\n",
    "    for post in posts:\n",
    "        word_positions = {}  # Dictionary to track positions of each word\n",
    "\n",
    "        # Track word positions\n",
    "        for idx, word in enumerate(post):\n",
    "            if word not in word_positions:\n",
    "                word_positions[word] = []\n",
    "            word_positions[word].append(idx)\n",
    "\n",
    "        # Count word frequencies\n",
    "        unique_words = set(post)  # Track unique words in the post to avoid double counting\n",
    "        for word in unique_words:\n",
    "            word_frequency[word] = word_frequency.get(word, 0) + 1\n",
    "\n",
    "        # Count word pair co-occurrences with distance check\n",
    "        for word1 in word_positions:\n",
    "            for word2 in word_positions:\n",
    "                if word1 != word2:\n",
    "                    # Check if the words are 5 or more positions apart\n",
    "                    for pos1 in word_positions[word1]:\n",
    "                        for pos2 in word_positions[word2]:\n",
    "                            if abs(pos1 - pos2) >= 5:\n",
    "                                word_pair_frequency[word1][word2] = word_pair_frequency[word1].get(word2, 0) + 1\n",
    "# Step 4: Calculate the lift between word pairs\n",
    "def calculate_lift(word_pairs):\n",
    "    \"\"\"\n",
    "    Calculates the lift between word pairs using the formula:\n",
    "    Lift(word1, word2) = P(word1 AND word2) / (P(word1) * P(word2))\n",
    "    Lift is written to the lift values CSV and stored in a DataFrame for further processing.\n",
    "    \"\"\"\n",
    "    global itr\n",
    "\n",
    "    for word1, word2 in word_pairs:\n",
    "        # Get the frequency of word1, word2, and their co-occurrence\n",
    "        freq_word1 = word_frequency.get(word1, 0)\n",
    "        freq_word2 = word_frequency.get(word2, 0)\n",
    "        co_occurrence = word_pair_frequency.get(word1, {}).get(word2, 0)\n",
    "\n",
    "        # Calculate probabilities\n",
    "        p_word1 = freq_word1 / file_length if freq_word1 else 0\n",
    "        p_word2 = freq_word2 / file_length if freq_word2 else 0\n",
    "        p_word1_and_word2 = co_occurrence / file_length if co_occurrence else 0\n",
    "\n",
    "        # Avoid division by zero\n",
    "        if p_word1 > 0 and p_word2 > 0:\n",
    "            lift_value = p_word1_and_word2 / (p_word1 * p_word2) if (p_word1 * p_word2) > 0 else 0\n",
    "        else:\n",
    "            lift_value = 0\n",
    "\n",
    "        # Store lift value in DataFrame\n",
    "        df_lift.loc[itr] = [word1, word2, lift_value]\n",
    "        itr += 1\n",
    "\n",
    "# Step 5: Write lift values and matrix to CSV\n",
    "def save_results():\n",
    "    \"\"\"\n",
    "    Writes the calculated lift values to a CSV file and also generates a lift matrix,\n",
    "    saving it to another CSV.\n",
    "    \"\"\"\n",
    "    # Save lift values DataFrame to CSV\n",
    "    df_lift.to_csv(output_lift_values, index=False)\n",
    "\n",
    "    # Generate lift matrix\n",
    "    lift_matrix = pd.pivot_table(df_lift, values='lift_value', index='word1', columns='word2', fill_value=0)\n",
    "    lift_matrix.to_csv(output_lift_matrix)\n",
    "\n",
    "# Main function to run all steps\n",
    "def main():\n",
    "    # Step 1: Load word pairs\n",
    "    word_pairs = load_word_pairs(pair_keys_file)\n",
    "\n",
    "    # Step 2: Process the input file to extract posts\n",
    "    posts = process_input_file(input_file)\n",
    "\n",
    "    # Step 3: Calculate frequencies\n",
    "    calculate_frequencies(posts)\n",
    "\n",
    "    # Step 4: Calculate lift values\n",
    "    calculate_lift(word_pairs)\n",
    "\n",
    "    # Step 5: Save results\n",
    "    save_results()\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc1d6ba-4485-473c-bd33-dfaf40769877",
   "metadata": {},
   "source": [
    "## Task D: MDS Map\n",
    "https://www.geeksforgeeks.org/sklearn-multi-dimensional-scaling-mds-python-implementation-from-scratch/\n",
    "\n",
    "https://github.com/tanyakuznetsova/Multidimensional-Scaling-of-European_Cities/blob/main/Multidimensional_scaling_of_European_Cities.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92cdbe-1728-4d78-ae9c-686ace8fefe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fe22dec-6b39-413c-b959-0f35167d9366",
   "metadata": {},
   "source": [
    "## Task E: Insights from C and D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa3239-a6ed-4c01-84e7-7f310fb5fa21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a884a735-11fe-48ca-9a4e-34cd7f32da40",
   "metadata": {},
   "source": [
    "## Task F: Car Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9154b8-3d26-4a34-86be-5f1ecc93410e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad780fba-6ae6-4402-a47a-13e9c00d9250",
   "metadata": {},
   "source": [
    "## Task G: Advice from F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf14e31-9bf3-4f73-8e70-867da7e021ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef14db79-a40a-49ad-96be-8c596fb82aa9",
   "metadata": {},
   "source": [
    "## Task H: Aspirational Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15527577-c77c-4767-a44c-135df365f4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
